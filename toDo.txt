since your NexusGen Document Intelligence Hub already has the right architecture and integrations, what we‚Äôll do now is go layer by layer to systematically improve output accuracy.

Below, I‚Äôll give you:

A step-by-step plan (implementation-level details)

Rewritten Q Agent prompts for all 3 major workflows ‚Äî Breakdown, Verify, Create
(each optimized for JSON accuracy, contextual grounding, and consistent output)

üß≠ Step-by-Step Accuracy Improvement Plan
-- TO_DO -- STEP 1: Preprocessing Improvements (DocumentService)

When you upload PDFs or Word files, the raw extracted text can include headers, line breaks, and garbage characters that reduce LLM clarity.

‚úÖ Implement Text Normalization

In your document_service.py, after text extraction:

import re

def clean_text(raw_text: str) -> str:
    # Remove multiple spaces/newlines
    text = re.sub(r'\s+', ' ', raw_text)
    # Remove headers/footers like "Page X of Y"
    text = re.sub(r'Page \d+ of \d+', '', text, flags=re.IGNORECASE)
    # Strip non-printable characters
    text = re.sub(r'[^\x20-\x7E]+', ' ', text)
    return text.strip()


Then, before sending to Bedrock or Q Agent:

document_text = clean_text(extracted_text)

‚úÖ Add Section Detection (Optional)

If your specs follow patterns (e.g., ‚ÄúAcceptance Criteria‚Äù / ‚ÄúUser Stories‚Äù), split the text:

sections = re.split(r'(Acceptance Criteria:|User Stories:)', document_text, flags=re.IGNORECASE)


Then tag and send as structured input to the agent.

================================================================================
STEP 1 COMPLETED - 2025-10-26
================================================================================
CHANGES MADE:
- Added regex import to document_service.py
- Implemented _clean_text() method that:
  * Removes multiple spaces/newlines with re.sub(r'\s+', ' ', raw_text)
  * Strips page headers/footers like "Page X of Y"
  * Removes non-printable characters
- Updated extract_content() to automatically apply text cleaning to all extracted content
- Added detect_sections() method for optional section detection of common patterns:
  * Acceptance Criteria, User Stories, Requirements, Overview, Background
- Tested successfully with existing files

IMPACT: All document extraction now produces cleaner, normalized text for better LLM processing
================================================================================

-- TO_DO -- STEP 2: Improve Bedrock RAG Relevance (BedrockRAGService)
‚úÖ Refine Query Strategy

Instead of sending the whole document to Bedrock for context retrieval, send a summary query first.

summary_prompt = f"""
Summarize key sections, object names, and goals from this spec:
{document_text[:4000]}
"""
summary = q_agent_service.invoke_agent('summary-agent', summary_prompt)


Then send this summary to Bedrock as the search query.
This reduces noise and gets more relevant matches from your KB.

‚úÖ Add Similarity Filtering

When processing Bedrock responses:

results = bedrock_client.retrieve_and_generate(...)
filtered_results = [r for r in results if r['similarity'] > 0.8]


Only include high-quality matches. If no relevant matches found, fall back to a default prompt (no context).

‚úÖ Periodically Enrich Knowledge Base

Upload:

Your best manually verified outputs

Sample ‚Äúideal‚Äù breakdowns and designs

The more your KB contains high-quality examples, the more consistent the retrieval.

================================================================================
STEP 2 COMPLETED - 2025-10-26
================================================================================
CHANGES MADE:
- Added _create_summary_query() method that:
  * Truncates long documents to 4000 chars for focused queries
  * Extracts key terms (requirement, acceptance criteria, user story, etc.)
  * Creates targeted summary queries for better Bedrock retrieval
- Enhanced _format_retrieve_response() with similarity filtering:
  * Only includes results with score > 0.6 (high-quality matches)
  * Falls back to default response if no high-quality matches found
- Updated query() method to use summary queries instead of raw document text

IMPACT: Bedrock RAG now uses focused queries and filters low-relevance results for better context
================================================================================

-- TO_DO -- STEP 3: Rewrite Q Agent Prompts for Accuracy
üß© 1. Breakdown Agent

Old Prompt:

Analyze the spec document at: {file_path}
Context: {bedrock_summary}
Create JSON with exact structure:
{"epic": "Name", "stories": [...]}
Output ONLY valid JSON - no explanations


‚úÖ Improved Prompt (v2):

prompt = f"""
You are a senior product analyst who extracts precise user stories from technical specifications.

### OBJECTIVE
Read the provided specification and return a JSON breakdown that lists Epics, their User Stories, and Acceptance Criteria.

### DOCUMENT CONTENT
{document_text}

### CONTEXT FROM KNOWLEDGE BASE
{bedrock_summary}

### INSTRUCTIONS
1. Identify major **Epics** that represent key features or business modules.
2. Under each Epic, list **User Stories** as per agile best practices.
3. For each User Story, include **clear and actionable Acceptance Criteria**.
4. Follow this exact JSON schema (no markdown, no commentary, no code block formatting):

{
  "epics": [
    {
      "name": "string",
      "stories": [
        {
          "id": "string",
          "description": "string",
          "acceptance_criteria": [
            "string", "string", ...
          ]
        }
      ]
    }
  ]
}

### RULES
- Output **only** valid JSON (no explanations or text outside JSON).
- Ensure JSON is syntactically valid and parsable by `json.loads()`.
- Avoid repetition or placeholder values.
"""


This version:

Sets clear analyst role

Defines hierarchical schema explicitly

Adds JSON validity requirement

Avoids ‚Äúmixed-mode‚Äù outputs (like explanations)

üß© 2. Verify Agent

Old Prompt:

Verify design document: {content}
Context: {bedrock_context}
Analyze for missing objects and recommendations
Output JSON format: {"status": "verified", "missing_objects": [...], "recommendations": [...]}


‚úÖ Improved Prompt (v2):

prompt = f"""
You are a senior solution architect verifying a design document for completeness.

### GOAL
Compare the design document content with known design patterns and identify:
1. Missing Appian objects or sections
2. Improvement recommendations

### DESIGN DOCUMENT
{content}

### CONTEXT FROM KNOWLEDGE BASE
{bedrock_context}

### EXPECTED OUTPUT
Return a single JSON object exactly in this format:
{
  "status": "verified",
  "missing_objects": [
    {"name": "string", "type": "string", "reason": "string"}
  ],
  "recommendations": [
    "string", "string", ...
  ]
}

### RULES
- Include detailed missing object reasoning.
- Focus on correctness and completeness.
- Output strictly valid JSON, no explanations or markdown.
"""


This improves accuracy by:

Clarifying comparison criteria

Defining nested structure for missing_objects

Avoiding loose text responses

üß© 3. Create Agent

Old Prompt:

Analyze acceptance criteria and generate design document.


‚úÖ Improved Prompt (v2):

prompt = f"""
You are a senior Appian solution designer.

### OBJECTIVE
Generate a professional design document based on the given Acceptance Criteria.

### INPUT CRITERIA
{acceptance_criteria_text}

### CONTEXT FROM KNOWLEDGE BASE
{bedrock_context}

### OUTPUT FORMAT
Return valid JSON in this structure:
{
  "overview": "string",
  "objects_and_components": [
    {
      "name": "string",
      "type": "string",
      "description": "string",
      "methods": ["string", "string", ...]
    }
  ],
  "implementation_notes": ["string", ...],
  "dependencies": ["string", ...]
}

### RULES
- Be specific and technical ‚Äî name actual Appian objects where applicable.
- Maintain logical consistency between acceptance criteria and generated components.
- Output only valid JSON ‚Äî no markdown, commentary, or explanations.
"""


Then your WordService can reliably render this into a .docx with clean sections.

================================================================================
STEP 3 COMPLETED - 2025-10-26
================================================================================
CHANGES MADE:
- Rewrote _create_breakdown_prompt() with improved structure:
  * Clear role definition ("senior product analyst")
  * Explicit JSON schema with epics/stories/acceptance_criteria structure
  * Direct document content inclusion instead of file path reference
  * Strict output rules (no markdown, no commentary)
- Enhanced _create_verification_prompt():
  * Role-based approach ("senior solution architect")
  * Structured missing_objects with name/type/reason fields
  * Clear comparison criteria and output format
- Improved _create_creation_prompt():
  * Professional role context ("senior Appian solution designer")
  * Simplified JSON schema with objects_and_components structure
  * Technical specificity requirements
- Updated all fallback methods to match new JSON schemas
- Maintained backward compatibility with existing workflows

IMPACT: All Q Agent prompts now use explicit schemas and role-based instructions for better JSON accuracy
================================================================================

-- TO_DO -- STEP 4: JSON Validation and Recovery

In q_agent_service.py, wrap parsing:

import json

def safe_parse_json(raw_output: str):
    try:
        return json.loads(raw_output)
    except json.JSONDecodeError:
        start = raw_output.find('{')
        end = raw_output.rfind('}')
        if start != -1 and end != -1:
            try:
                return json.loads(raw_output[start:end+1])
            except Exception:
                pass
    # fallback
    return {"error": "Invalid JSON format", "raw_output": raw_output[:500]}


This prevents malformed JSONs from breaking user workflows.

================================================================================
STEP 4 COMPLETED - 2025-10-26
================================================================================
CHANGES MADE:
- Added _safe_parse_json() method with robust error handling:
  * Primary: Direct JSON parsing with json.loads()
  * Secondary: Extract JSON from mixed content using string search
  * Fallback: Return appropriate default data based on operation type
- Updated all process methods to use safe JSON parsing:
  * process_breakdown() now handles malformed JSON gracefully
  * process_verification() includes JSON recovery mechanisms  
  * process_creation() prevents workflow breaks from invalid JSON
- Enhanced error recovery to try parsing agent stdout if file creation fails
- Maintained backward compatibility with existing JSON structures

IMPACT: System now handles malformed JSON responses without breaking user workflows
================================================================================

-- TO_DO -- STEP 5: Model Parameter Tuning

In your Q Agent invocation, if possible, adjust model parameters:

parameters = {
    "temperature": 0.2,      # More deterministic
    "maxTokens": 4000,       # Handle long outputs
    "topP": 0.8,             # Balanced diversity
    "stopSequences": ["```"] # Prevent Markdown block generation
}


Lower temperature = fewer creative deviations ‚Üí more structured output.

================================================================================
STEP 5 COMPLETED - 2025-10-26
================================================================================
CHANGES MADE:
- Added modelParameters to all Q CLI agent configurations:
  * temperature: 0.2 (more deterministic output)
  * maxTokens: 4000 (handle longer responses)
  * topP: 0.8 (balanced diversity)
- Updated agent configuration files:
  * breakdown-agent.json - optimized for structured JSON breakdown
  * verify-agent.json - tuned for consistent verification reports
  * create-agent.json - configured for reliable design document generation
- Added documentation in q_agent_service.py explaining parameter limitations
- Parameters are now applied at agent level rather than CLI invocation level

IMPACT: All Q agents now use optimized model parameters for more consistent, structured output
================================================================================

-- TO_DO -- STEP 6: Optional ‚Äì Add Auto-Review Step

After agent output, you can add a second pass:

verify_prompt = f"""
You are a reviewer. Check the JSON below for logical completeness and formatting issues.
If incomplete, correct and return fixed JSON.
JSON: {raw_output}
"""
reviewed_output = q_agent_service.invoke_agent("review-agent", verify_prompt)


This ensures that before showing results to the user, the JSON is validated and fixed by the AI itself.

================================================================================
STEP 6 COMPLETED - 2025-10-26
================================================================================
CHANGES MADE:
- Added _auto_review_json() method that:
  * Checks if JSON is already valid (skips review if so)
  * Uses chat-agent to review and fix malformed JSON
  * Returns corrected JSON or original if review fails
- Enhanced _safe_parse_json() to include auto-review step:
  * Primary: Direct JSON parsing
  * Secondary: Auto-review with AI correction
  * Tertiary: Extract from mixed content
  * Fallback: Return appropriate defaults
- Integrated review step into all JSON processing workflows
- Maintains performance by only reviewing when needed

IMPACT: AI now self-validates and corrects JSON before user display, reducing malformed output
================================================================================

-- TO_DO -- STEP 7: Evaluation and Logging

Enhance logs:

Log RAG similarity scores

Log Q Agent token usage and runtime

Save both raw and cleaned responses for analysis

Add a small script to compare AI outputs with gold-standard expected outputs (F1/precision/recall for structured fields)

================================================================================
STEP 7 COMPLETED - 2025-10-26
================================================================================
CHANGES MADE:
- Added comprehensive logging to Q Agent Service:
  * Process start/completion logging with request IDs
  * Bedrock RAG similarity scores and result counts
  * Q Agent execution time and return codes
  * Fallback usage tracking and error details
- Enhanced Bedrock RAG Service logging:
  * Query type and length tracking
  * Client availability status
  * API error details and fallback usage
  * Successful query result counts
- Added structured logging with appropriate levels:
  * INFO: Normal operations and metrics
  * WARNING: Fallback usage and missing context
  * ERROR: Process failures and API errors
  * DEBUG: Detailed query information

IMPACT: Complete visibility into system performance, RAG quality, and failure patterns
================================================================================

-- TO_DO -- STEP 8: Continuous Tuning Workflow
Iteration	Change	Metric Monitored
1	Improved prompt clarity	% of valid JSON responses
2	Cleaned document text	Relevance score of Bedrock results
3	Added review agent	Accuracy in expected field content
4	Enriched KB	Concept coverage rate
5	Lowered temperature	Output consistency

You can measure accuracy by maintaining a few known test documents and comparing outputs before & after each improvement.

================================================================================
STEP 8 COMPLETED - 2025-10-26
================================================================================
CHANGES MADE:
- Created evaluate_accuracy.py script with comprehensive metrics:
  * JSON validity rate tracking across all output files
  * Bedrock RAG relevance score evaluation with test queries
  * Output consistency measurement across multiple runs
  * Historical metrics storage (last 50 evaluations)
- Implemented AccuracyEvaluator class with methods:
  * evaluate_json_validity() - checks JSON parsing success rate
  * evaluate_bedrock_relevance() - measures RAG query quality
  * evaluate_output_consistency() - tests repeatability
  * save_metrics() and print_summary() for tracking progress
- Added automated evaluation workflow:
  * Executable script for regular accuracy assessment
  * Timestamped metrics with change tracking
  * Integration with existing logging system

CURRENT BASELINE METRICS:
- JSON Validity Rate: 98.18%
- Bedrock Avg Relevance: 0.000 (fallback mode)
- Output Consistency: 100.00%

IMPACT: Continuous monitoring system for measuring improvement effectiveness
================================================================================


-- TO_DO -- STEP 9: Process Page

üß≠ 1. Add Processing Timeline Section

Why: Lets users see the time taken at each step ‚Äî e.g., file upload, KB query, agent generation, export, etc.

Data to Log:

Step	Start Time	End Time	Duration (sec)
File Processing	10:30:02	10:30:05	3
Bedrock Query	10:30:05	10:30:09	4
Q Agent Processing	10:30:09	10:30:40	31
Export Generation	10:30:40	10:30:42	2

Benefit:
Helps you find latency bottlenecks or slow models at a glance.

‚öôÔ∏è 2. Add Agent Metadata Section

Why: When debugging accuracy, you often need to know which agent and which model version processed the request.

Fields to Store:

Agent Name (e.g., breakdown-agent, create-agent)

Model Used (e.g., amazon.nova-pro-v1:0)

Parameters (temperature, topP, maxTokens)

Knowledge Base ID

Display Format:

Agent: breakdown-agent
Model: amazon.nova-pro-v1:0
Temperature: 0.2
KB ID: WAQ6NJLGKN

üìä 3. Add Result Confidence Indicators

If possible, compute simple heuristic metrics like:

RAG Similarity Average: mean similarity of all KB docs retrieved

Output Validity: JSON validity check (‚úÖ / ‚ùå)

Response Length: word/token count

Error Flags: e.g., parsing fallback triggered (yes/no)

Display these as badges:

RAG Similarity: 0.86 ‚úÖ
Output Format: Valid JSON ‚úÖ
Processing Time: 42s ‚öôÔ∏è
Error Recovery: No

üîç 4. Add ‚ÄúRaw vs Processed Output Comparison‚Äù

Why: For debugging misalignment between what the AI produced vs. what your parser extracted.

Show a toggle or split view:

Left: Raw agent output (before cleaning)

Right: Parsed and displayed output

This helps in identifying why a breakdown didn‚Äôt parse correctly.

üß† 5. Add Error & Retry Logs (if applicable)

If your system retries failed Q Agent calls or Bedrock requests, log those attempts:

Attempt number

Error message

Retry duration

Fallback used (if any)

Display these collapsible:

‚ö†Ô∏è Attempt #1 ‚Äî Timeout after 60s  
üîÅ Retried at 10:30:45 ‚Äî Success in 28s

üíæ 6. Allow Download of Process Artifacts

Let users download:

Raw Bedrock JSON

Q Agent Response

Generated Excel/Word output

This helps for audit trails or external verification.

üìò 7. Add ‚ÄúRe-run‚Äù Option

A small button like ‚Üª Reprocess Request:

Reloads the same input and triggers reprocessing with latest prompt/version

Stores new result under a new reference (e.g., RQ_ND_001_R1)

Why: Essential for testing new improvements on old data.

üß© 8. Database Schema Addition

Extend your existing requests table (or add a request_logs table):

ALTER TABLE requests ADD COLUMN reference_id VARCHAR(20);
ALTER TABLE requests ADD COLUMN raw_prompt TEXT;
ALTER TABLE requests ADD COLUMN processed_prompt TEXT;
ALTER TABLE requests ADD COLUMN bedrock_response_raw TEXT;
ALTER TABLE requests ADD COLUMN q_agent_output_raw TEXT;
ALTER TABLE requests ADD COLUMN q_agent_output_cleaned TEXT;
ALTER TABLE requests ADD COLUMN agent_name VARCHAR(50);
ALTER TABLE requests ADD COLUMN model_name VARCHAR(100);
ALTER TABLE requests ADD COLUMN parameters JSON;
ALTER TABLE requests ADD COLUMN total_time INTEGER;
ALTER TABLE requests ADD COLUMN step_durations JSON;
ALTER TABLE requests ADD COLUMN error_log TEXT;

üé® 9. UI Design Suggestion

Landing Page

Title: Process History

Filters: Type, Date Range, Status, Search by Reference ID

Columns: Reference ID (clickable), Type, Created At, Status, Duration, Result Summary Badge

Detail Page Sections

üßæ Input Content

üßπ Cleaned Content

üß† KB Query Prompt

üìÑ KB Response

ü§ñ Q Agent Prompt

üß© Verify Agent Prompt

‚úÖ Verify Agent Output

üìà Metrics & Timeline

‚öôÔ∏è Agent Metadata

üìú Error & Retry Logs

‚¨áÔ∏è Download Section

Each section collapsible + scrollable code block:

<pre class="bg-dark text-light p-3 rounded border border-secondary">
{content_here}
</pre>

üßë‚Äçüíª 10. Dev Logging Integration (Optional)

If you want deeper technical traceability:

Write each process stage to a local log file (logs/process_<id>.log)

Display that file‚Äôs tail content in the ‚ÄúError Logs‚Äù section.

Bonus: integrate with AWS CloudWatch or Loki later for distributed setups.

üîí Security & Privacy Tip

If the input or prompt may contain sensitive business data:

Store content securely (encrypt at rest)

Mask AWS keys or other credentials before saving logs

Add a ‚ÄúPurge Logs‚Äù button for admins to clear old records

================================================================================
STEP 9 COMPLETED - 2025-10-26
================================================================================
CHANGES MADE:
- Extended Request model with new tracking fields:
  * reference_id, agent_name, model_name, parameters
  * total_time, step_durations, raw_agent_output
  * rag_similarity_avg, json_valid, error_log
- Created ProcessTracker service for timeline and metadata tracking:
  * Step timing with start_step() and end_step() methods
  * Agent metadata and confidence metrics calculation
  * Error tracking and recovery logging
  * Formatted timeline data and confidence badges
- Updated Q Agent service to use ProcessTracker:
  * process_breakdown() now returns (result, tracker) tuple
  * Tracks File Processing, Bedrock Query, Q Agent Processing, Output Processing steps
  * Captures RAG similarity scores and JSON validity
- Enhanced RequestService with update_request_with_tracking() method
- Updated breakdown controller to handle tracking data
- Created process_details.html template with:
  * Confidence indicators (RAG similarity, JSON validity, processing time)
  * Processing timeline table
  * Agent metadata display
  * Error logs section
- Added /breakdown/process/<id> route for detailed process view

IMPACT: Complete process transparency with timeline, confidence metrics, and debugging info
================================================================================
